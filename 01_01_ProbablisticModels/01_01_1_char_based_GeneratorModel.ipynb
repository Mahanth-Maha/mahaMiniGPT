{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-based Generative model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527204238, str)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Valkyria Chronicles III Senj no Valkyria 3 : Unrecorded Chronicles ( Japanese : 3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer R'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/mini_gpt.txt', 'r') as f:\n",
    "    X_train_data = f.read()\n",
    "\n",
    "len(X_train_data) , type(X_train_data)\n",
    "X_train_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025171, str)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Homarus gammarus Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . Description Homarus gammarus is a large crustacean , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 6 kilograms ( 11 13 lb ) , although the lobsters caught in lobster pots are usually 23 38 cm ( 9 15 in ) long and weigh 0 7 2 2 kg ( 1 5 4 9 lb ) . Like other '"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/valid_mini_gpt.txt', 'r') as f:\n",
    "    X_valid_data = f.read()\n",
    "\n",
    "len(X_valid_data) , type(X_valid_data)\n",
    "X_valid_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258729, str)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Robert Boulter Robert Boulter is an English film , television and theatre actor . He had a guest starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/test_mini_gpt.txt', 'r') as f:\n",
    "    X_test_data = f.read()\n",
    "\n",
    "len(X_test_data) , type(X_test_data)\n",
    "X_test_data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding - Decoding\n",
    "\n",
    "### Basic mapper as coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' !\"#$%&\\'()*+,-./0123456789:;<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~',\n",
       " 94)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''.join(set(X_train_data))\n",
    "coding_str = sorted(s)\n",
    "''.join(coding_str) , len(coding_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' !\"#$%&\\'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^abcdefghijklmnopqrstuvwxyz',\n",
       " 86)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(' !\"$%&\\'()*+,-./0123456789:;<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz~',\n",
       " 86)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''.join(set(X_test_data))\n",
    "''.join(sorted(s)) , len(s)\n",
    "s = ''.join(set(X_valid_data))\n",
    "''.join(sorted(s)) , len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = {}\n",
    "decoder = {}\n",
    "for i, char in enumerate(coding_str): \n",
    "    encoder[char] = i\n",
    "    decoder[i] = char\n",
    "\n",
    "\n",
    "encoder['M']\n",
    "decoder[encoder['M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 64, 71, 64, 77, 83, 71]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Mahanth'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda x: [encoder[char] for char in x]\n",
    "encode('Mahanth')\n",
    "decode = lambda x: ''.join([decoder[i] for i in x])\n",
    "decode(encode('Mahanth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tikt = tiktoken.get_encoding('gpt2')\n",
    "tikt.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 19210, 400]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Mahanth'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tikt.encode('Mahanth')\n",
    "tikt.decode(tikt.encode('Mahanth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Encoding into a ```TENSOR``` Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(encode(X_train_data) , dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([527204238]), torch.Size([1258729]), torch.Size([1025171]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = torch.tensor(encode(X_test_data) , dtype = torch.long)\n",
    "X_valid= torch.tensor(encode(X_valid_data) , dtype = torch.long)\n",
    "\n",
    "X_train.shape , X_test.shape , X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([527204238]),\n",
       " torch.int64,\n",
       " tensor([53, 64, 75, 74, 88, 81, 72, 64,  0, 34]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train.dtype , X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53, 64, 75, 74, 88, 81, 72, 64,  0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Valkyria '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "X_train[:block_size + 1]\n",
    "X_train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[75, 64, 83, 72, 78, 77, 82, 71],\n",
       "         [82, 66, 78, 81, 83, 68, 67,  0],\n",
       "         [72, 79, 64, 83, 68, 67,  0, 82],\n",
       "         [81, 68, 64, 82, 68,  0, 72, 77]]),\n",
       " tensor([[64, 83, 72, 78, 77, 82, 71, 72],\n",
       "         [66, 78, 81, 83, 68, 67,  0, 65],\n",
       "         [79, 64, 83, 68, 67,  0, 82, 68],\n",
       "         [68, 64, 82, 68,  0, 72, 77,  0]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_split( on = 'train',batch_size = batch_size , block_size = block_size , X_train = X_train,X_test = X_test,X_valid = X_valid):\n",
    "    if on == 'train':\n",
    "        X = X_train\n",
    "    elif on == 'test':\n",
    "        X = X_test\n",
    "    else:\n",
    "        X = X_valid\n",
    "    ix = torch.randint(len(X) - block_size , (batch_size,) )\n",
    "    x = [X[i : i + block_size] for i in ix]\n",
    "    y = [X[i + 1 : i + block_size + 1] for i in ix]\n",
    "    return torch.stack(x) , torch.stack(y) \n",
    "\n",
    "x,y = batch_split()\n",
    "x.shape , y.shape \n",
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Bi Gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 94]), tensor(5.2589, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size , vocab_size)\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        logits = self.token_embedding_table(x) # (batch_size, block_size, vocab_size)\n",
    "        if y is None:\n",
    "            return logits\n",
    "        \n",
    "        # entropy expects : (N, C) input : (batch_size * block_size, vocab_size)\n",
    "        loss = F.cross_entropy(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, n_pred):\n",
    "        for _ in range(n_pred):\n",
    "            logits = self(x)[:,-1,:]\n",
    "            prob_dist = F.softmax(logits, -1)\n",
    "            x = torch.cat([x, torch.multinomial(prob_dist, 1)], -1)\n",
    "            \n",
    "            # logits = self.token_embedding_table(x)\n",
    "            # x = torch.cat([x, torch.argmax(logits, -1)[:,-1].unsqueeze(-1)], -1)\n",
    "        return x\n",
    "\n",
    "m = BiGramLanguageModel(len(coding_str))\n",
    "logits,loss = m(x,y)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random giberish text generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' J\\\\u#fJ};9J<mU/\\\\IGv~r9u[4HU0`hK6d&Q\\\\F@2Yw<;v|OKCAp>$KLOMuf$c6k>Cko/vv@:seyIMGcIBTOitDQTO(T\"H%+iy]r~s-'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.zeros((1,1), dtype = torch.long)\n",
    "decode(m.generate( x1 , n_pred = 100)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "using Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(m.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.4655206203460693\n",
      "Loss : 2.448436975479126\n",
      "Loss : 2.3681063652038574\n",
      "Loss : 2.506385564804077\n",
      "Loss : 2.8326637744903564\n",
      "Loss : 2.7357940673828125\n",
      "Loss : 2.620405912399292\n",
      "Loss : 2.4185690879821777\n",
      "Loss : 2.194861888885498\n",
      "Loss : 3.183865785598755\n",
      "Loss : 2.260883092880249\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for i in range(10000):\n",
    "    x,y = batch_split()\n",
    "    optimiser.zero_grad(set_to_none = True)\n",
    "    _, loss = m(x,y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Loss : {loss.item()}')\n",
    "        # print(decode(m.generate(x[:1], n_pred = 100)[0].tolist()))\n",
    "print(f'Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' And thid tore blthesintof t . rting thathertrir aptathenuce , . ravee a h to Fil duatth aticalearn oto acerngaves 246 c , s Theraton nd caplyommasthoro . tismenofrwatrasthiorcegh preldy cha . Hea ted lpongte e , dinck sers rirred ofat ivinitiolls pss'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.zeros((1,1), dtype = torch.long)\n",
    "decode(m.generate( x1 , n_pred = 250)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' talurstoff 3 abe . coralid l jan orok s r an Whid pleis , Meis l as blllyd o s wasig isitby ointh Berarmecio iou ran amayinarsipt \" wigured in thinngeparoy Pl s Jutsced trsply heeded ftonithe isch Chittorfocecedertorthon ia cl sahed mpathed ppl d , lise sbuly cuanun sthen t miper . to , les B . rinoung ancelehac s n pesing fed P eas , wn serar . iff Ralem jedins min bes thes , ak tin , Uns Borigle lltandeoveblfonintahart : Is peapas marediceurinugede , ctnano , tins prn cag s , Sce s sty aratrmeulol , \" Nathevene rylls whts Founid iccas far sshimor to pral Thed If s m vedelere tore Ha whyerviovickaiurs thit mintially tour gun Bangld s fialed f s Stll milatersthenonune amied hedan Therinedatra Pondipar otendinthenug inger Ind by s Th frereafforerecon ne , o y y Hum \" fooredal be n . bes an balanth cand wo th , wioaralicangecriomet En the fatil fe t re iltrllend nd d . Thastir oy bubredey wre , pectoll s d obat theles ro azo opth toorbl aroumipicerenaceson . Fre dec anthariampsoldenghoun'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0:00:00.195715\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "x1 = torch.zeros((1,1), dtype = torch.long)\n",
    "decode(m.generate( x1 , n_pred = 1000)[0].tolist())\n",
    "k = datetime.datetime.now() - start_time\n",
    "\n",
    "print(f'Time taken : {k}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc",
   "language": "python",
   "name": "iisc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
