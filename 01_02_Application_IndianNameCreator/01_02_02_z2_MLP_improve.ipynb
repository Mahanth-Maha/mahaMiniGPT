{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barjraj',\n",
       " 'ramdin',\n",
       " 'verma',\n",
       " 'sharat',\n",
       " 'chandran',\n",
       " 'birender',\n",
       " 'mandal',\n",
       " 'amit',\n",
       " 'kushal',\n",
       " 'kasid']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open('./data/indian_names/indian_names.csv').read().split('\\n')\n",
    "names = [name for name in names if len(name) > 0]\n",
    "words = []\n",
    "for n in names:\n",
    "    words += n.split(' ')\n",
    "words = [w for w in words if len(w) > 3] \n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kumar', 1779),\n",
       " ('singh', 1159),\n",
       " ('devi', 968),\n",
       " ('kumari', 551),\n",
       " ('pooja', 455),\n",
       " ('sharma', 431),\n",
       " ('jyoti', 249),\n",
       " ('deepak', 215),\n",
       " ('sunita', 202),\n",
       " ('rahul', 188)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "k = Counter(words).items()\n",
    "sorted(Counter(words).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abhi', 1),\n",
       " ('paal', 1),\n",
       " ('axat', 1),\n",
       " ('jony', 1),\n",
       " ('azaz', 1),\n",
       " ('molu', 1),\n",
       " ('jang', 1),\n",
       " ('naag', 1),\n",
       " ('vude', 1),\n",
       " ('shsi', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(k, key=lambda x: (x[1], len(x[0])))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.abcdefghijklmnopqrstuvwxyz', 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = '.' + ''.join(sorted(set(''.join(words))))\n",
    "voc_size = len(voc)\n",
    "voc , voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc(c):\n",
    "    return voc.index(c)\n",
    "\n",
    "def dec(i):\n",
    "    return voc[i]\n",
    "\n",
    "def make_dataset(words, block_size = 3):\n",
    "    X , y = [], []\n",
    "    for word in words:\n",
    "        word = '.'*block_size + word + '.'\n",
    "        for i in range(len(word) - block_size):\n",
    "            X.append([enc(k) for k in word[i:i+block_size]])\n",
    "            y.append(enc(word[i+block_size]))\n",
    "    return torch.tensor(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/indian_names/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# Xtr,  ytr  = make_dataset(words[:n1] , block_size=3)     # 80%\n",
    "# Xdev, ydev = make_dataset(words[n1:n2] , block_size=3)   # 10%\n",
    "# Xte,  yte  = make_dataset(words[n2:] , block_size=3)     # 10%\n",
    "\n",
    "# torch.save(Xtr, data_folder + 'Xtr_bs_3.pt')\n",
    "# torch.save(ytr, data_folder + 'ytr_bs_3.pt')\n",
    "# torch.save(Xdev, data_folder + 'Xdev_bs_3.pt')\n",
    "# torch.save(ydev, data_folder + 'ydev_bs_3.pt')\n",
    "# torch.save(Xte, data_folder + 'Xte_bs_3.pt')\n",
    "# torch.save(yte, data_folder + 'yte_bs_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = torch.load( data_folder + 'Xtr_bs_3.pt')\n",
    "ytr = torch.load( data_folder + 'ytr_bs_3.pt')\n",
    "Xdev = torch.load( data_folder + 'Xdev_bs_3.pt')\n",
    "ydev = torch.load( data_folder + 'ydev_bs_3.pt')\n",
    "Xte = torch.load( data_folder + 'Xte_bs_3.pt')\n",
    "yte = torch.load( data_folder + 'yte_bs_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([217230, 3]), torch.Size([27277, 3]), torch.Size([27270, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xdev.shape, Xte.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, inputs, dimensions, block_size, hidden, outputs):\n",
    "        self.C  = torch.randn(inputs, dimensions, requires_grad=True) \n",
    "        self.W1 = torch.randn(dimensions * block_size, hidden, requires_grad=True)\n",
    "        self.b1 = torch.randn(hidden, requires_grad=True)\n",
    "        self.W2 = torch.randn(hidden, outputs, requires_grad=True)\n",
    "        self.b2 = torch.randn(outputs, requires_grad=True)\n",
    "        self.parameters =  [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "        self.dimensions = dimensions\n",
    "        self.block_size = block_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "        emb = self.C[X]\n",
    "        h = torch.tanh(emb.view(-1, self.dimensions*self.block_size) @ self.W1 + self.b1)\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def backward(self,logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            logits = self.forward(X[mini_batch])\n",
    "            loss = self.backward(logits,y[mini_batch])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "            \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no of parameters : 6097'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "dimensions = 10\n",
    "\n",
    "mlp100 = MLP(\n",
    "    inputs = voc_size, \n",
    "    dimensions = dimensions,\n",
    "    block_size = block_size,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size\n",
    ")\n",
    "f'no of parameters : {mlp100.get_params_count()}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 20.96150\n",
      "iter = 10000, loss = 2.17068\n",
      "iter = 20000, loss = 1.69685\n",
      "iter = 30000, loss = 1.76626\n",
      "iter = 40000, loss = 1.46695\n",
      "iter = 50000, loss = 1.51726\n",
      "iter = 60000, loss = 1.39342\n",
      "iter = 70000, loss = 1.27919\n",
      "iter = 80000, loss = 1.52634\n",
      "iter = 90000, loss = 1.54958\n",
      "iter = 100000, loss = 1.89278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8928, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = \t\t1.61364\n",
      "validation loss = \t1.64401\n"
     ]
    }
   ],
   "source": [
    "print(f'train loss = \\t\\t{mlp100.get_loss(Xtr, ytr).item():.5f}\\nvalidation loss = \\t{mlp100.get_loss(Xdev, ydev).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.64191734790802"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:  \n",
    "Initial loss is 19.44517, but if every character is uniformly random, the loss should be 1/voc_size = 1/27 = 0.0370370370 \n",
    "\n",
    "### Cause :\n",
    "softmax confidently wrong\n",
    "\n",
    "the softmax function is squashing down slighlty higher values to +1/-1 's making the most of the losses going up, hence higher loss initally and takes unnecessarily long time to bring it down\n",
    "\n",
    "### Sol: W2 init\n",
    "decrease the weights by multiplying them with say $\\lambda = 0.1$, which may keep most of the values in the range of tanh and keeps the outputs in -1 to 1 and reduce the loss for outler layer weights $W_2$ and $b_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, inputs, dimensions, block_size, hidden, outputs, delta = 0.1):\n",
    "        self.C  = torch.randn(inputs, dimensions) \n",
    "        self.W1 = torch.randn(dimensions * block_size, hidden)\n",
    "        self.b1 = torch.randn(hidden)\n",
    "        self.W2 = torch.randn(hidden, outputs) * delta\n",
    "        self.b2 = torch.randn(outputs) * delta\n",
    "        self.parameters =  [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "        self.dimensions = dimensions\n",
    "        self.block_size = block_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "        \n",
    "    def forward(self, X):\n",
    "        emb = self.C[X]\n",
    "        h = torch.tanh(emb.view(-1, self.dimensions*self.block_size) @ self.W1 + self.b1)\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def backward(self,logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            logits = self.forward(X[mini_batch])\n",
    "            loss = self.backward(logits,y[mini_batch])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p.retain_grad()\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "            \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp100 = MLP(\n",
    "    inputs = voc_size, \n",
    "    dimensions = dimensions,\n",
    "    block_size = block_size,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    delta = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.87594\n",
      "iter = 10000, loss = 1.69253\n",
      "iter = 20000, loss = 1.73381\n",
      "iter = 30000, loss = 1.58649\n",
      "iter = 40000, loss = 1.65464\n",
      "iter = 50000, loss = 1.51102\n",
      "iter = 60000, loss = 2.01614\n",
      "iter = 70000, loss = 1.99297\n",
      "iter = 80000, loss = 1.76886\n",
      "iter = 90000, loss = 1.28250\n",
      "iter = 100000, loss = 1.78882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.7888, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = \t\t1.60924\n",
      "validation loss = \t1.64067\n"
     ]
    }
   ],
   "source": [
    "mlp100.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)\n",
    "print(f'train loss = \\t\\t{mlp100.get_loss(Xtr, ytr).item():.5f}\\nvalidation loss = \\t{mlp100.get_loss(Xdev, ydev).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6404290199279785"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "some what solved as loss started at 3.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  \n",
    "saturated tanh\n",
    "\n",
    "### Cause :\n",
    "the tanh function is squashing down slighlty higher values to +1/-1 's making the most of the losses going up, hence higher loss initally and takes unnecessarily long time to bring it down\n",
    "\n",
    "### Sol: W1 init\n",
    "decrease the weights by multiplying them with $\\lambda = 0.1$, which may keep most of the values in the range of tanh and keeps the outputs in -1 to 1 and reduce the loss for $W_1$ and $b_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:  \n",
    "But can we decide or derive an proper number to multiply with rather than guessing ?\n",
    "\n",
    "### Sol: Kamming init \n",
    "\n",
    "multiply weights with :$\\frac{gain}{fan mode}$\n",
    "\n",
    "* for tanh : gain is $\\frac{5}{3}$  hence $\\frac{\\frac{5}{3}}{\\sqrt{fan_{in}}} $\n",
    "\n",
    "* for ReLU : gain is $\\sqrt{2}$  hence $\\frac{\\sqrt{2}}{\\sqrt{fan_{in}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, inputs, dimensions, block_size, hidden, outputs, delta = 0.1):\n",
    "        kammin_init = (5/3) / ((dimensions * block_size) ** 0.5) \n",
    "        self.C  = torch.randn(inputs, dimensions) \n",
    "        self.W1 = torch.randn(dimensions * block_size, hidden) * kammin_init\n",
    "        self.b1 = torch.randn(hidden) * kammin_init\n",
    "        self.W2 = torch.randn(hidden, outputs) * delta\n",
    "        self.b2 = torch.randn(outputs) * delta\n",
    "        self.parameters =  [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "        self.dimensions = dimensions\n",
    "        self.block_size = block_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "        \n",
    "    def forward(self, X):\n",
    "        emb = self.C[X]\n",
    "        h = torch.tanh(emb.view(-1, self.dimensions*self.block_size) @ self.W1 + self.b1)\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def backward(self,logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            logits = self.forward(X[mini_batch])\n",
    "            loss = self.backward(logits,y[mini_batch])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p.retain_grad()\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "            \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.53868\n",
      "iter = 10000, loss = 1.21541\n",
      "iter = 20000, loss = 1.51428\n",
      "iter = 30000, loss = 1.53383\n",
      "iter = 40000, loss = 1.35891\n",
      "iter = 50000, loss = 1.83586\n",
      "iter = 60000, loss = 1.34913\n",
      "iter = 70000, loss = 1.23212\n",
      "iter = 80000, loss = 1.03981\n",
      "iter = 90000, loss = 1.63102\n",
      "iter = 100000, loss = 1.11431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1143, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = \t\t1.58817\n",
      "validation loss = \t1.62430\n"
     ]
    }
   ],
   "source": [
    "mlp100 = MLP(\n",
    "    inputs = voc_size, \n",
    "    dimensions = dimensions,\n",
    "    block_size = block_size,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    delta = 0.1\n",
    ")\n",
    "mlp100.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)\n",
    "print(f'train loss = \\t\\t{mlp100.get_loss(Xtr, ytr).item():.5f}\\nvalidation loss = \\t{mlp100.get_loss(Xdev, ydev).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6205750703811646"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "We had a better minimized loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:  \n",
    "should we init these weights **precisely always** ?\n",
    "\n",
    "No, As Modren innovations doesn't require these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sol 4.1: Batch Normalisation \n",
    "paper : [arXiv:1502.03167](https://arxiv.org/abs/1502.03167)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.39818\n",
      "iter = 10000, loss = 1.56852\n",
      "iter = 20000, loss = 1.83559\n",
      "iter = 30000, loss = 2.00934\n",
      "iter = 40000, loss = 1.44431\n",
      "iter = 50000, loss = 1.60379\n",
      "iter = 60000, loss = 1.83953\n",
      "iter = 70000, loss = 1.31751\n",
      "iter = 80000, loss = 2.00082\n",
      "iter = 90000, loss = 1.45018\n",
      "iter = 100000, loss = 1.88489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8849, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = \t\t1.59163\n",
      "validation loss = \t1.61244\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, inputs, dimensions, block_size, hidden, outputs, delta = 0.1):\n",
    "        self.inputs = inputs\n",
    "        self.dimensions = dimensions\n",
    "        self.block_size = block_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        kammin_init = (5/3) / ((dimensions * block_size) ** 0.5) \n",
    "\n",
    "        self.C  = torch.randn(self.inputs, self.dimensions) \n",
    "        self.W1 = torch.randn(self.dimensions * self.block_size, hidden) * kammin_init\n",
    "        # we no need b1 here as it gets canceled in batch normalization and grad is zero\n",
    "        # also we can use gamma and beta to scale and shift the output of the batch normalization\n",
    "        # self.b1 = torch.randn(self.hidden) * kammin_init \n",
    "        self.W2 = torch.randn(self.hidden, self.outputs) * delta\n",
    "        self.b2 = torch.randn(self.outputs) * delta\n",
    "        self.gamma = torch.ones((1,self.hidden))\n",
    "        self.beta = torch.zeros((1,self.hidden))\n",
    "\n",
    "        self.parameters =  [\n",
    "            self.C, \n",
    "            self.W1, \n",
    "            # self.b1, \n",
    "            self.W2, \n",
    "            self.b2 , \n",
    "            self.gamma, \n",
    "            self.beta\n",
    "        ]\n",
    "        \n",
    "\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "        \n",
    "    def forward(self, X):\n",
    "        emb = self.C[X]\n",
    "        h = emb.view(-1, self.dimensions*self.block_size) @ self.W1 # + self.b1) # batch normalization will cancel this anyway\n",
    "        \n",
    "        # batch NORMALIZATION\n",
    "        h = (h - h.mean(dim=0 , keepdim=True)) / (h.std(dim=0 , keepdim=True) + self.epsilon) \n",
    "        \n",
    "        # scale and shift\n",
    "        h = h * self.gamma + self.beta\n",
    "        h = torch.tanh(h)\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def backward(self,logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            logits = self.forward(X[mini_batch])\n",
    "            loss = self.backward(logits,y[mini_batch])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p.retain_grad()\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "            \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n",
    "\n",
    "\n",
    "mlp100 = MLP(\n",
    "    inputs = voc_size, \n",
    "    dimensions = dimensions,\n",
    "    block_size = block_size,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    delta = 0.1\n",
    ")\n",
    "mlp100.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)\n",
    "print(f'train loss = \\t\\t{mlp100.get_loss(Xtr, ytr).item():.5f}\\nvalidation loss = \\t{mlp100.get_loss(Xdev, ydev).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.606314778327942"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sol 4.2 : Batch Norm with buffers\n",
    "paper : [arXiv:1502.03167](https://arxiv.org/abs/1502.03167)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.48807\n",
      "iter = 10000, loss = 1.91980\n",
      "iter = 20000, loss = 1.43032\n",
      "iter = 30000, loss = 1.91190\n",
      "iter = 40000, loss = 1.53877\n",
      "iter = 50000, loss = 1.70825\n",
      "iter = 60000, loss = 1.47717\n",
      "iter = 70000, loss = 1.44520\n",
      "iter = 80000, loss = 1.80084\n",
      "iter = 90000, loss = 1.96703\n",
      "iter = 100000, loss = 1.32094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3209, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = \t\t1.59625\n",
      "validation loss = \t1.61634\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, inputs, dimensions, block_size, hidden, outputs, delta = 0.1):\n",
    "        self.inputs = inputs\n",
    "        self.dimensions = dimensions\n",
    "        self.block_size = block_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        kammin_init = (5/3) / ((dimensions * block_size) ** 0.5) \n",
    "\n",
    "        self.C  = torch.randn(self.inputs, self.dimensions) \n",
    "        self.W1 = torch.randn(self.dimensions * self.block_size, hidden) * kammin_init\n",
    "        # we no need b1 here as it gets canceled in batch normalization and grad is zero\n",
    "        # also we can use bn_mean and bn_std to scale and shift the output of the batch normalization\n",
    "        # self.b1 = torch.randn(self.hidden) * kammin_init \n",
    "        self.W2 = torch.randn(self.hidden, self.outputs) * delta\n",
    "        self.b2 = torch.randn(self.outputs) * delta\n",
    "        \n",
    "        self.bn_mean = torch.ones((1,self.hidden))\n",
    "        self.bn_std = torch.zeros((1,self.hidden))\n",
    "\n",
    "        self.parameters =  [\n",
    "            self.C, \n",
    "            self.W1, \n",
    "            # self.b1, \n",
    "            self.W2, \n",
    "            self.b2 , \n",
    "            self.bn_mean, \n",
    "            self.bn_std\n",
    "        ]\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        self.mean_running = torch.zeros((1,self.hidden))\n",
    "        self.std_running = torch.ones((1,self.hidden))\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''linear layer'''\n",
    "        emb = self.C[X]\n",
    "        h = emb.view(-1, self.dimensions*self.block_size) @ self.W1 # + self.b1) # batch normalization will cancel this anyway\n",
    "        \n",
    "        '''batch NORMALIZATION layer'''\n",
    "        h_mean = h.mean(dim=0 , keepdim=True)\n",
    "        h_std = h.std(dim=0 , keepdim=True)\n",
    "        h = ( (h - h_mean ) / (h_std + self.epsilon) ) * self.bn_mean + self.bn_std\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            retain_rate = 0.999\n",
    "            self.mean_running = retain_rate * self.mean_running + (1-retain_rate) * h_mean\n",
    "            self.std_running = retain_rate * self.std_running + (1-retain_rate) * h_std\n",
    "        \n",
    "        '''Non linear layer'''\n",
    "        h = torch.tanh(h)\n",
    "\n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "    \n",
    "    def backward(self,logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            logits = self.forward(X[mini_batch])\n",
    "            loss = self.backward(logits,y[mini_batch])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p.retain_grad()\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "            \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n",
    "\n",
    "\n",
    "mlp100 = MLP(\n",
    "    inputs = voc_size, \n",
    "    dimensions = dimensions,\n",
    "    block_size = block_size,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    delta = 0.1\n",
    ")\n",
    "mlp100.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)\n",
    "print(f'train loss = \\t\\t{mlp100.get_loss(Xtr, ytr).item():.5f}\\nvalidation loss = \\t{mlp100.get_loss(Xdev, ydev).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6165716648101807"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.get_loss(Xte, yte).item() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "decrease in loss and we have also got mean and std as buffers to compute for a single example rather than a batch for generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0, 18],\n",
       "        [ 0, 18,  1],\n",
       "        [18,  1, 22],\n",
       "        [ 1, 22,  9]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myalla\\AppData\\Local\\Temp\\ipykernel_32876\\2245753419.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mlp100.forward(torch.tensor(Xtr[:5]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4881,  2.5852,  0.8258, -0.1989,  1.8182, -2.7224,  0.2606,  0.9202,\n",
       "          1.2704, -2.2601, -0.6772,  1.3893,  2.3911,  2.2940,  2.1166, -2.3313,\n",
       "          1.5539, -5.3507,  1.8201,  4.9462, -0.7500, -1.9031,  0.5241, -2.1375,\n",
       "         -4.6880, -2.0004, -3.2685],\n",
       "        [-1.1312,  6.3215, -1.1254, -1.6310, -0.5798,  3.8771,  0.3257, -1.1302,\n",
       "         -3.4537,  4.1822,  1.5650,  0.7608, -2.2792, -0.0647, -2.1940,  4.8553,\n",
       "          0.1314, -0.3759, -0.8690,  0.2150, -0.5535,  3.7717,  1.1698,  0.0871,\n",
       "         -3.9299, -1.2475, -4.5861],\n",
       "        [-2.4576, -0.9401, -0.1559, -0.3427,  2.5042, -3.3280, -2.1423,  0.9801,\n",
       "          2.6135, -1.6980,  3.2226,  0.7955,  0.4424,  2.7477,  1.6422, -4.2669,\n",
       "         -1.9138, -4.7681,  0.2883,  1.4695,  0.6522, -1.5704,  2.0444, -0.0095,\n",
       "         -2.5141, -1.5463, -1.4503],\n",
       "        [ 6.2657,  3.4087, -0.6471, -1.1638, -0.1980,  5.5320, -4.3471, -1.9057,\n",
       "          1.2012,  8.4168,  0.9023, -0.5622, -2.5121, -0.7826,  2.4607,  1.8017,\n",
       "         -0.9573, -3.8729,  1.4384, -1.3773, -1.4702,  3.7536, -1.1022, -0.3451,\n",
       "         -3.1665,  1.1319, -1.8613],\n",
       "        [ 2.7212,  2.0472, -0.5172, -1.2605, -0.7379, -0.6953, -1.1044, -2.4717,\n",
       "         -1.0635, -0.0665, -0.3654,  0.0606,  1.3101, -0.7584,  5.5879,  0.1056,\n",
       "         -1.2472, -3.0962,  2.6166,  3.6011,  5.9630, -1.8458,  1.2794,  1.5985,\n",
       "         -6.3166,  2.5010, -1.4075]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp100.forward(torch.tensor(Xtr[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH-IFYING\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TORCH.NN.LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fin, fout, bias = True):\n",
    "        self.weight = torch.randn(fin, fout) / fin**0.5\n",
    "        self.bias_exist = bias\n",
    "        self.bias = torch.randn(fout) / fin**0.5 if bias else None \n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias] if self.bias_exist is not None else [self.weight]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out  = x @self.weight \n",
    "        if self.bias_exist:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "class BatchNormal1D:\n",
    "    def __init__(self, dim , epsilon = 1e-5, momentum = 0.01 ):\n",
    "        self.dim = dim \n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.training = True\n",
    "        \n",
    "        self.gamma = torch.ones((1,self.dim))\n",
    "        self.beta = torch.zeros((1,self.dim))\n",
    "\n",
    "        self.mean_running = torch.zeros((1,self.dim))\n",
    "        self.std_running = torch.ones((1,self.dim))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training: \n",
    "            xmean = x.mean(dim=0 , keepdim=True)\n",
    "            xstd = x.std(dim=0 , keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                self.mean_running = (1-self.momentum) * self.mean_running + self.momentum * xmean\n",
    "                self.std_running = (1-self.momentum) * self.std_running + self.momentum * xstd\n",
    "        else :\n",
    "            xmean = self.mean_running\n",
    "            xstd = self.std_running\n",
    "        \n",
    "        self.out = ( (x - xmean ) / (xstd + self.epsilon) ) * self.gamma + self.beta\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = voc_size\n",
    "dims = 10\n",
    "b_size =3\n",
    "hidden = 100\n",
    "outputs = voc_size\n",
    "\n",
    "C = torch.randn(voc_size, dims) \n",
    "layers = [\n",
    "    Linear(dims*b_size, hidden),    \n",
    "    # BatchNormal1D(hidden),          \n",
    "    Tanh(),\n",
    "\n",
    "    Linear(hidden, hidden),         \n",
    "    # BatchNormal1D(hidden),          \n",
    "    Tanh(),\n",
    "\n",
    "    Linear(hidden, hidden),         \n",
    "    # BatchNormal1D(hidden),          \n",
    "    Tanh(),\n",
    "\n",
    "    Linear(hidden, outputs),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= (5/3) # all inside (embedding) layers weight kammin init (solution 3)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias *= (5/3)\n",
    "\n",
    "    layers[-1].weight *= 0.1   # last layer weight init (solution 2)\n",
    "\n",
    "# init weights \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26297"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.31261\n",
      "iter = 1000, loss = 2.32022\n",
      "iter = 2000, loss = 1.96970\n",
      "iter = 3000, loss = 1.84116\n",
      "iter = 4000, loss = 2.02868\n",
      "iter = 5000, loss = 2.27159\n",
      "iter = 6000, loss = 1.82919\n",
      "iter = 7000, loss = 1.67293\n",
      "iter = 8000, loss = 2.09506\n",
      "iter = 9000, loss = 1.63244\n",
      "iter = 10000, loss = 1.83713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8371, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd( X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "    for iter in range(max_iters+1):\n",
    "        mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "\n",
    "        emb = C[X[mini_batch]]\n",
    "        h = emb.view(-1, dims*b_size)\n",
    "        for layer in layers:\n",
    "            h = layer(h)\n",
    "        loss = F.cross_entropy(h, y[mini_batch])\n",
    "        \n",
    "        for layer in layers:\n",
    "            layer.out.retain_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in parameters:\n",
    "                p.retain_grad()\n",
    "                p -= p.grad * alpha\n",
    "                p.grad = None\n",
    "        \n",
    "        if iter > max_iters *0.95:\n",
    "            alpha = alpha / 100\n",
    "        elif iter > max_iters *0.9:\n",
    "            alpha = alpha / 10\n",
    "        \n",
    "        if verbose and iter % (max_iters/10) == 0:\n",
    "            print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "sgd(Xtr, ytr, alpha = 0.1, max_iters = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Class for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnMLP:\n",
    "    def __init__(self,inputs, dims , b_size, hidden, outputs, n_layers):\n",
    "        self.inputs = inputs\n",
    "        self.dims = dims\n",
    "        self.b_size = b_size\n",
    "        self.hidden = hidden\n",
    "        self.outputs = outputs\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.C = torch.randn(self.inputs, self.dims) \n",
    "        self.layers = []\n",
    "        self.layers.append(Linear(self.dims*self.b_size, self.hidden))\n",
    "        # self.layers.append(BatchNormal1D(self.hidden))\n",
    "        self.layers.append(Tanh()) \n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.layers.append(Linear(self.hidden, self.hidden))\n",
    "            # self.layers.append(BatchNormal1D(self.hidden))\n",
    "            self.layers.append(Tanh()) \n",
    "        self.layers.append(Linear(self.hidden, self.outputs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for layer in self.layers[:-1]:\n",
    "                if isinstance(layer, Linear):\n",
    "                    layer.weight *= (5/3) # all inside (embedding) layers weight kammin init (solution 3)\n",
    "                    if layer.bias is not None:\n",
    "                        layer.bias *= (5/3)\n",
    "\n",
    "            self.layers[-1].weight *= 0.1   # last layer weight init (solution 2)\n",
    "\n",
    "        self.parameters = [self.C] + [p for layer in self.layers for p in layer.parameters()]\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        emb = self.C[X]\n",
    "        h = emb.view(-1, self.dims*self.b_size)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        # h.requires_grad = True\n",
    "        return h \n",
    "\n",
    "    def backward(self, logits, y):\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        # for layer in self.layers:\n",
    "        #     layer.out.retain_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    def get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def sgd(self, X, y, alpha = 0.1 , batching_size = 32, max_iters = 1000, verbose = True):\n",
    "        for iter in range(max_iters+1):\n",
    "            mini_batch = torch.randint(0, X.shape[0], (batching_size,))\n",
    "            # logits = self.forward(X[mini_batch])\n",
    "            # loss = self.backward(logits,y[mini_batch])\n",
    "            emb = self.C[X[mini_batch]]\n",
    "            h = emb.view(-1, self.dims*self.b_size)\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "            loss = F.cross_entropy(h, y[mini_batch])\n",
    "            for layer in self.layers:\n",
    "                layer.out.retain_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters:\n",
    "                    p.retain_grad()\n",
    "                    p -= p.grad * alpha\n",
    "                    p.grad = None\n",
    "                \n",
    "            if iter > max_iters *0.95:\n",
    "                alpha = alpha / 100\n",
    "            elif iter > max_iters *0.9:\n",
    "                alpha = alpha / 10\n",
    "            \n",
    "            if verbose and iter % (max_iters/10) == 0:\n",
    "                print(f'{iter = }, loss = {loss.item():.5f}')\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_params_count(self):\n",
    "        return sum([p.numel() for p in self.parameters])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16197"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.3195345401763916"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.32930\n",
      "iter = 10000, loss = 1.54642\n",
      "iter = 20000, loss = 1.28443\n",
      "iter = 30000, loss = 1.26273\n",
      "iter = 40000, loss = 1.78174\n",
      "iter = 50000, loss = 1.47434\n",
      "iter = 60000, loss = 1.47202\n",
      "iter = 70000, loss = 0.88945\n",
      "iter = 80000, loss = 1.27649\n",
      "iter = 90000, loss = 1.24785\n",
      "iter = 100000, loss = 1.63052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6305, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp = BnMLP(\n",
    "    inputs = voc_size, \n",
    "    dims = 10,\n",
    "    b_size = 3,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    n_layers = 2\n",
    ")\n",
    "\n",
    "bnmlp.get_params_count()\n",
    "bnmlp.get_loss(Xtr, ytr).item()\n",
    "bnmlp.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5304666757583618"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.571450114250183"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.576159119606018"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp.get_loss(Xtr, ytr).item()\n",
    "bnmlp.get_loss(Xdev, ydev).item()\n",
    "bnmlp.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nuatri'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, start = '', max_len = 50):\n",
    "    word = '.'*model.b_size + start\n",
    "    for i in range(max_len):\n",
    "        x = [enc(k) for k in word[-model.b_size:]]\n",
    "        x = torch.tensor([x])\n",
    "        logits = model.forward(x)\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        word += dec(torch.multinomial(p[0], 1).item())\n",
    "        if word[-1] == '.':\n",
    "            break\n",
    "    return word[model.b_size : -1]\n",
    "\n",
    "generate(bnmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azurav\n",
      "manil\n",
      "shad\n",
      "shya\n",
      "jyoti\n",
      "ajay\n",
      "sima\n",
      "ahmeena\n",
      "shu\n",
      "deep\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surav\n",
      "shandania\n",
      "ala\n",
      "anjal\n",
      "jan\n",
      "budhardeepike\n",
      "kumar\n",
      "aaram\n",
      "shakunti\n",
      "taush\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abhaa\n",
      "prya\n",
      "sonika\n",
      "radav\n",
      "vipinku\n",
      "ari\n",
      "chansrat\n",
      "shi\n",
      "suraju\n",
      "deep\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with block size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([217526, 4]), torch.Size([27115, 4]), torch.Size([27136, 4]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# Xtr,  ytr  = make_dataset(words[:n1] , block_size=4)     # 80%\n",
    "# Xdev, ydev = make_dataset(words[n1:n2] , block_size=4)   # 10%\n",
    "# Xte,  yte  = make_dataset(words[n2:] , block_size=4)     # 10%\n",
    "\n",
    "# torch.save(Xtr, data_folder + 'Xtr_bs_4.pt')\n",
    "# torch.save(ytr, data_folder + 'ytr_bs_4.pt')\n",
    "# torch.save(Xdev, data_folder + 'Xdev_bs_4.pt')\n",
    "# torch.save(ydev, data_folder + 'ydev_bs_4.pt')\n",
    "# torch.save(Xte, data_folder + 'Xte_bs_4.pt')\n",
    "# torch.save(yte, data_folder + 'yte_bs_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = torch.load( data_folder + 'Xtr_bs_4.pt')\n",
    "ytr = torch.load( data_folder + 'ytr_bs_4.pt')\n",
    "Xdev = torch.load( data_folder + 'Xdev_bs_4.pt')\n",
    "ydev = torch.load( data_folder + 'ydev_bs_4.pt')\n",
    "Xte = torch.load( data_folder + 'Xte_bs_4.pt')\n",
    "yte = torch.load( data_folder + 'yte_bs_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([217526, 4]), torch.Size([27115, 4]), torch.Size([27136, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xdev.shape, Xte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17197"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp = BnMLP(\n",
    "    inputs = voc_size, \n",
    "    dims = 10,\n",
    "    b_size = 4,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    n_layers = 2\n",
    ")\n",
    "\n",
    "bnmlp.get_params_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2590951919555664"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp.get_loss(Xtr, ytr).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.31006\n",
      "iter = 10000, loss = 1.24934\n",
      "iter = 20000, loss = 1.14485\n",
      "iter = 30000, loss = 1.46663\n",
      "iter = 40000, loss = 1.70179\n",
      "iter = 50000, loss = 1.58840\n",
      "iter = 60000, loss = 1.47759\n",
      "iter = 70000, loss = 1.26664\n",
      "iter = 80000, loss = 1.60378\n",
      "iter = 90000, loss = 1.56357\n",
      "iter = 100000, loss = 1.40011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4001, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.394792079925537"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.4625132083892822"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.4424569606781006"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp.get_loss(Xtr, ytr).item()\n",
    "bnmlp.get_loss(Xdev, ydev).item()\n",
    "bnmlp.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kumari\n",
      "pooja\n",
      "kanchal\n",
      "irti\n",
      "tinkar\n",
      "sureshma\n",
      "sharma\n",
      "dikshankaj\n",
      "nitipal\n",
      "ashok\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singh\n",
      "sunder\n",
      "sahni\n",
      "radhan\n",
      "deep\n",
      "tanve\n",
      "rout\n",
      "kumarkash\n",
      "devi\n",
      "aashan\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kumar\n",
      "pandeepak\n",
      "salma\n",
      "deepak\n",
      "dibashida\n",
      "kanchika\n",
      "sunita\n",
      "bharamjeet\n",
      "ajay\n",
      "deen\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27297"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp2 = BnMLP(\n",
    "    inputs = voc_size, \n",
    "    dims = 10,\n",
    "    b_size = 4,\n",
    "    hidden = 100,\n",
    "    outputs = voc_size,\n",
    "    n_layers = 3\n",
    ")\n",
    "\n",
    "bnmlp2.get_params_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 3.34225\n",
      "iter = 10000, loss = 1.76189\n",
      "iter = 20000, loss = 1.30926\n",
      "iter = 30000, loss = 1.53168\n",
      "iter = 40000, loss = 1.75249\n",
      "iter = 50000, loss = 1.29392\n",
      "iter = 60000, loss = 1.07405\n",
      "iter = 70000, loss = 1.58285\n",
      "iter = 80000, loss = 1.69981\n",
      "iter = 90000, loss = 1.49916\n",
      "iter = 100000, loss = 1.44929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4493, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp2.sgd(Xtr, ytr, alpha = 0.1, max_iters = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3560278415679932"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.4276996850967407"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.42019522190094"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmlp2.get_loss(Xtr, ytr).item()\n",
    "bnmlp2.get_loss(Xdev, ydev).item()\n",
    "bnmlp2.get_loss(Xte, yte).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeeender\n",
      "singh\n",
      "kumar\n",
      "kumar\n",
      "resailasha\n",
      "dhary\n",
      "kalurvna\n",
      "amit\n",
      "mavaoo\n",
      "kavita\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinki\n",
      "simran\n",
      "sunita\n",
      "tosh\n",
      "swagtiktru\n",
      "varshadiya\n",
      "arora\n",
      "saima\n",
      "ravinashok\n",
      "savita\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sibharti\n",
      "sartka\n",
      "kumar\n",
      "devi\n",
      "singh\n",
      "kallu\n",
      "pinki\n",
      "chandey\n",
      "sureshrrer\n",
      "deepali\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(bnmlp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13139"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gayam\n",
      "ravind\n",
      "taramnesh\n",
      "rajputida\n",
      "kuldeepa\n"
     ]
    }
   ],
   "source": [
    "len(set(names))\n",
    "wasted_trails = 0\n",
    "for i in range(5):\n",
    "    gens = generate(bnmlp2)\n",
    "    while gens in names:\n",
    "        wasted_trails += 1\n",
    "        gens = generate(bnmlp2)\n",
    "    print(gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc",
   "language": "python",
   "name": "iisc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
