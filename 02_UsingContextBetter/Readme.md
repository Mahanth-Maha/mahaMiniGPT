# Using Context in better ways -  Attention mechanisms

## Version 1: Averaging context 
> Similar to what RNN/LSTM/GRU will produce as latent after encoding, but here there is no positional encoding as well

* loss of the positional information, \& averaging makes it generic 

## Version 2: Self Attention
> Pass